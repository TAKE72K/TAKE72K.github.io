<!DOCTYPE html>
<html lang="zh-tw">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='Chip Placement with Deep Reinforcement Learning 閱讀筆記  link
 引言 Google這篇一出可說語驚四座，從未有人把強化學習拿來擺physical design中的macro過。
事實上，這個步驟可說是整個flow中複雜度最高的部分之一，原因無他，所有的改變都牽一髮而動全身，要完美衡量擺放macro的效果去做決定，不能說不可能，但肯定是相當耗時的。
這時工程師真正的作用就出來了，經驗老到的可能一看就知道怎樣擺(可能)比較好，過去我們開發的演算法，可說是在用程式模擬這種經驗；而強化學習則另闢蹊徑，讓AI來扮演菜鳥工程師，從零開始學習這些know-how。
相關研究 Global placment問題是晶片設計上的一大挑戰，前人所提出的方法大致可以歸納為三個方向：基於分區、基於隨機優化、基於分析。
方法 問題定義 這份研究的課題是晶片擺置最佳化，目標是將netlist(描述晶片的graph)這張圖的節點定位在2D的有限平面上，也就是擺置空間，並且同時最佳化Power, performance and area (PPA)。
方法概覽 為了與深度強化學習做結合，必須將這個擺置問題轉化為Markov Decision Processes (MDPs)的形式，由四個要素構成：
 states: 所有可能發生的環境的集合 (也就是所有可能產生的擺置)。 actions: 智慧型代理人能執行的動作的集合 (也就是擺放macro到一個合法位置)。 state transition: 給定state和action，轉場到下一個state。 reward: 在某個state做某個動作的獎勵 (設定為零，擺完最後一步之後會用估計線長與擁擠狀況的負加權合作為獎勵。)  我們將$s_0$定義為初始state，也就是空無一物的平面，而最後的終點則是$s_T$，也就是擺放了所有Macro後完整的擺置結果。在每個單位時間$t$，智慧型代理人會觀察當前的環境$s_t$，並做動作$a_t$，以到達下一個階段$s_{t&#43;1}$，同時獲得獎勵$r_t$ ($t&amp;lt;T$時為0，$t=T$時為估計的cost)。
透過重複學習這些紀錄(episodes)，智慧型代理人能透過策略網路(policy network)學習最大化cost的方法。這篇論文使用Proximal Policy Optimization (PPO)的方式，來更新策略網路 $\pi_\theta(a|s)$中的參數$\theta$。
獎勵 (reward) 這篇論文的目標是最佳化PPA，而這些資訊通常是由商業tool在flow的最後產生，也就是report QoR，包含線長、繞線擁擠狀況、timing、密度、面積、power等等，這些資訊綜合在一起便是最精確的reward。然而這通常得花上數小時甚至一天的時間來產生這些結果，RL需要快速迭代的特性注定讓使用這樣的reward不現實可行，RL需要的是一個能快速估算的reward、同時足夠精確而可以代表最終的QoR。
這篇論文採用線長與繞線擁擠度來估計擺置品質，並將其加權相加後當作reward，也就是$r_t=-HPWL-c*Congestion$。
同時為了保證在足夠短的時間能算出線長，論文做了幾個假設：
 將標準邏輯閘用hMETIS叢集起來，再用forced-directed方式來快速擺置，這使得計算線長時能獲得快速而不失精確的標準邏輯閘線長。 將平面離散化成數千個格子，並將macro與cell叢集的重心放置於格子的中心。 計算標準邏輯閘時，假設所有的線都是來自叢集中心。 計算繞線壅擠時，只考慮前10%嚴重的格子。  線長 HPWL，常見的估算方法，其意義可以視為是繞線長度所需的下限，大約等於其Steiner tree的線長。 這篇論文並加入一個normalize factor $q(i)$來增加估算的精確性，隨著netlist中結點的數量增加HPWL，如下所示，其中$N_{netlist}$代表netlist的數量：'><title>Chip Placement with Deep Reinforcement Learning 閱讀筆記</title>

<link rel='canonical' href='https://take72k.github.io/EDA/placementByRL/'>

<link rel="stylesheet" href="/scss/style.min.css"><meta property='og:title' content='Chip Placement with Deep Reinforcement Learning 閱讀筆記'>
<meta property='og:description' content='Chip Placement with Deep Reinforcement Learning 閱讀筆記  link
 引言 Google這篇一出可說語驚四座，從未有人把強化學習拿來擺physical design中的macro過。
事實上，這個步驟可說是整個flow中複雜度最高的部分之一，原因無他，所有的改變都牽一髮而動全身，要完美衡量擺放macro的效果去做決定，不能說不可能，但肯定是相當耗時的。
這時工程師真正的作用就出來了，經驗老到的可能一看就知道怎樣擺(可能)比較好，過去我們開發的演算法，可說是在用程式模擬這種經驗；而強化學習則另闢蹊徑，讓AI來扮演菜鳥工程師，從零開始學習這些know-how。
相關研究 Global placment問題是晶片設計上的一大挑戰，前人所提出的方法大致可以歸納為三個方向：基於分區、基於隨機優化、基於分析。
方法 問題定義 這份研究的課題是晶片擺置最佳化，目標是將netlist(描述晶片的graph)這張圖的節點定位在2D的有限平面上，也就是擺置空間，並且同時最佳化Power, performance and area (PPA)。
方法概覽 為了與深度強化學習做結合，必須將這個擺置問題轉化為Markov Decision Processes (MDPs)的形式，由四個要素構成：
 states: 所有可能發生的環境的集合 (也就是所有可能產生的擺置)。 actions: 智慧型代理人能執行的動作的集合 (也就是擺放macro到一個合法位置)。 state transition: 給定state和action，轉場到下一個state。 reward: 在某個state做某個動作的獎勵 (設定為零，擺完最後一步之後會用估計線長與擁擠狀況的負加權合作為獎勵。)  我們將$s_0$定義為初始state，也就是空無一物的平面，而最後的終點則是$s_T$，也就是擺放了所有Macro後完整的擺置結果。在每個單位時間$t$，智慧型代理人會觀察當前的環境$s_t$，並做動作$a_t$，以到達下一個階段$s_{t&#43;1}$，同時獲得獎勵$r_t$ ($t&amp;lt;T$時為0，$t=T$時為估計的cost)。
透過重複學習這些紀錄(episodes)，智慧型代理人能透過策略網路(policy network)學習最大化cost的方法。這篇論文使用Proximal Policy Optimization (PPO)的方式，來更新策略網路 $\pi_\theta(a|s)$中的參數$\theta$。
獎勵 (reward) 這篇論文的目標是最佳化PPA，而這些資訊通常是由商業tool在flow的最後產生，也就是report QoR，包含線長、繞線擁擠狀況、timing、密度、面積、power等等，這些資訊綜合在一起便是最精確的reward。然而這通常得花上數小時甚至一天的時間來產生這些結果，RL需要快速迭代的特性注定讓使用這樣的reward不現實可行，RL需要的是一個能快速估算的reward、同時足夠精確而可以代表最終的QoR。
這篇論文採用線長與繞線擁擠度來估計擺置品質，並將其加權相加後當作reward，也就是$r_t=-HPWL-c*Congestion$。
同時為了保證在足夠短的時間能算出線長，論文做了幾個假設：
 將標準邏輯閘用hMETIS叢集起來，再用forced-directed方式來快速擺置，這使得計算線長時能獲得快速而不失精確的標準邏輯閘線長。 將平面離散化成數千個格子，並將macro與cell叢集的重心放置於格子的中心。 計算標準邏輯閘時，假設所有的線都是來自叢集中心。 計算繞線壅擠時，只考慮前10%嚴重的格子。  線長 HPWL，常見的估算方法，其意義可以視為是繞線長度所需的下限，大約等於其Steiner tree的線長。 這篇論文並加入一個normalize factor $q(i)$來增加估算的精確性，隨著netlist中結點的數量增加HPWL，如下所示，其中$N_{netlist}$代表netlist的數量：'>
<meta property='og:url' content='https://take72k.github.io/EDA/placementByRL/'>
<meta property='og:site_name' content='竹林七閑'>
<meta property='og:type' content='article'><meta property='article:section' content='Posts' /><meta property='article:tag' content='eda' /><meta property='article:tag' content='AI' /><meta property='article:tag' content='Reinforcement Learning' /><meta property='article:published_time' content='2022-11-03T21:01:00&#43;08:00'/><meta property='article:modified_time' content='2022-11-03T21:01:00&#43;08:00'/>
<meta name="twitter:site" content="@hyes94107">
    <meta name="twitter:creator" content="@hyes94107"><meta name="twitter:title" content="Chip Placement with Deep Reinforcement Learning 閱讀筆記">
<meta name="twitter:description" content="Chip Placement with Deep Reinforcement Learning 閱讀筆記  link
 引言 Google這篇一出可說語驚四座，從未有人把強化學習拿來擺physical design中的macro過。
事實上，這個步驟可說是整個flow中複雜度最高的部分之一，原因無他，所有的改變都牽一髮而動全身，要完美衡量擺放macro的效果去做決定，不能說不可能，但肯定是相當耗時的。
這時工程師真正的作用就出來了，經驗老到的可能一看就知道怎樣擺(可能)比較好，過去我們開發的演算法，可說是在用程式模擬這種經驗；而強化學習則另闢蹊徑，讓AI來扮演菜鳥工程師，從零開始學習這些know-how。
相關研究 Global placment問題是晶片設計上的一大挑戰，前人所提出的方法大致可以歸納為三個方向：基於分區、基於隨機優化、基於分析。
方法 問題定義 這份研究的課題是晶片擺置最佳化，目標是將netlist(描述晶片的graph)這張圖的節點定位在2D的有限平面上，也就是擺置空間，並且同時最佳化Power, performance and area (PPA)。
方法概覽 為了與深度強化學習做結合，必須將這個擺置問題轉化為Markov Decision Processes (MDPs)的形式，由四個要素構成：
 states: 所有可能發生的環境的集合 (也就是所有可能產生的擺置)。 actions: 智慧型代理人能執行的動作的集合 (也就是擺放macro到一個合法位置)。 state transition: 給定state和action，轉場到下一個state。 reward: 在某個state做某個動作的獎勵 (設定為零，擺完最後一步之後會用估計線長與擁擠狀況的負加權合作為獎勵。)  我們將$s_0$定義為初始state，也就是空無一物的平面，而最後的終點則是$s_T$，也就是擺放了所有Macro後完整的擺置結果。在每個單位時間$t$，智慧型代理人會觀察當前的環境$s_t$，並做動作$a_t$，以到達下一個階段$s_{t&#43;1}$，同時獲得獎勵$r_t$ ($t&amp;lt;T$時為0，$t=T$時為估計的cost)。
透過重複學習這些紀錄(episodes)，智慧型代理人能透過策略網路(policy network)學習最大化cost的方法。這篇論文使用Proximal Policy Optimization (PPO)的方式，來更新策略網路 $\pi_\theta(a|s)$中的參數$\theta$。
獎勵 (reward) 這篇論文的目標是最佳化PPA，而這些資訊通常是由商業tool在flow的最後產生，也就是report QoR，包含線長、繞線擁擠狀況、timing、密度、面積、power等等，這些資訊綜合在一起便是最精確的reward。然而這通常得花上數小時甚至一天的時間來產生這些結果，RL需要快速迭代的特性注定讓使用這樣的reward不現實可行，RL需要的是一個能快速估算的reward、同時足夠精確而可以代表最終的QoR。
這篇論文採用線長與繞線擁擠度來估計擺置品質，並將其加權相加後當作reward，也就是$r_t=-HPWL-c*Congestion$。
同時為了保證在足夠短的時間能算出線長，論文做了幾個假設：
 將標準邏輯閘用hMETIS叢集起來，再用forced-directed方式來快速擺置，這使得計算線長時能獲得快速而不失精確的標準邏輯閘線長。 將平面離散化成數千個格子，並將macro與cell叢集的重心放置於格子的中心。 計算標準邏輯閘時，假設所有的線都是來自叢集中心。 計算繞線壅擠時，只考慮前10%嚴重的格子。  線長 HPWL，常見的估算方法，其意義可以視為是繞線長度所需的下限，大約等於其Steiner tree的線長。 這篇論文並加入一個normalize factor $q(i)$來增加估算的精確性，隨著netlist中結點的數量增加HPWL，如下所示，其中$N_{netlist}$代表netlist的數量：">
    <link rel="shortcut icon" href="/ico/sword.png" />

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-131269345-3', 'auto');
	
	ga('send', 'pageview');
}
</script><style>
    :root {
        --sys-font-family: "New Tegomin", -apple-system, BlinkMacSystemFont, "Segoe UI", "Droid Sans", "Helvetica Neue";
        --article-font-family: "Noto Serif TC", var(--base-font-family);
    }
</style>

<script>
		(function () {
		    const customFont = document.createElement('link');
		    customFont.href = "https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&&family=Sawarabi+Mincho&family=New+Tegomin&display=swap";
		
		    customFont.type = "text/css";
		    customFont.rel = "stylesheet";
		
		    document.head.appendChild(customFont);
		}());
</script>
    </head>
    <body class="
    article-page has-toc
">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "dark");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex 
    
        extended
    
">
    
        <div id="article-toolbar">
            <a href="https://take72k.github.io" class="back-home">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



                <span>Back</span>
            </a>
        </div>
    
<main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/EDA/" >
                EDA
            </a>
        
    </header>
    

    <h2 class="article-title">
        <a href="/EDA/placementByRL/">Chip Placement with Deep Reinforcement Learning 閱讀筆記</a>
    </h2>

    

    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Nov 03, 2022</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    1 minute read
                </time>
            </div>
        
    </footer>
    
</div>
</header>

    <section class="article-content">
    <h1 id="chip-placement-with-deep-reinforcement-learning-閱讀筆記">Chip Placement with Deep Reinforcement Learning 閱讀筆記</h1>
<blockquote>
<p><a class="link" href="https://arxiv.org/abs/2004.10746"  target="_blank" rel="noopener"
    >link</a></p>
</blockquote>
<h2 id="引言">引言</h2>
<p>Google這篇一出可說語驚四座，從未有人把強化學習拿來擺physical design中的macro過。<br>
事實上，這個步驟可說是整個flow中複雜度最高的部分之一，原因無他，所有的改變都牽一髮而動全身，要完美衡量擺放macro的效果去做決定，不能說不可能，但肯定是相當耗時的。<br>
這時工程師真正的作用就出來了，經驗老到的可能一看就知道怎樣擺(可能)比較好，過去我們開發的演算法，可說是在用程式模擬這種經驗；而強化學習則另闢蹊徑，讓AI來扮演菜鳥工程師，從零開始學習這些know-how。</p>
<h2 id="相關研究">相關研究</h2>
<p>Global placment問題是晶片設計上的一大挑戰，前人所提出的方法大致可以歸納為三個方向：基於分區、基於隨機優化、基於分析。</p>
<h2 id="方法">方法</h2>
<h3 id="問題定義">問題定義</h3>
<p>這份研究的課題是晶片擺置最佳化，目標是將netlist(描述晶片的graph)這張圖的節點定位在2D的有限平面上，也就是擺置空間，並且同時最佳化Power, performance and area (PPA)。</p>
<h3 id="方法概覽">方法概覽</h3>
<p>為了與深度強化學習做結合，必須將這個擺置問題轉化為Markov Decision Processes (MDPs)的形式，由四個要素構成：</p>
<ul>
<li>states: 所有可能發生的環境的集合 (也就是所有可能產生的擺置)。</li>
<li>actions: 智慧型代理人能執行的動作的集合 (也就是擺放macro到一個合法位置)。</li>
<li>state transition: 給定state和action，轉場到下一個state。</li>
<li>reward: 在某個state做某個動作的獎勵 (設定為零，擺完最後一步之後會用估計線長與擁擠狀況的負加權合作為獎勵。)</li>
</ul>
<p>我們將$s_0$定義為初始state，也就是空無一物的平面，而最後的終點則是$s_T$，也就是擺放了所有Macro後完整的擺置結果。在每個單位時間$t$，智慧型代理人會觀察當前的環境$s_t$，並做動作$a_t$，以到達下一個階段$s_{t+1}$，同時獲得獎勵$r_t$ ($t&lt;T$時為0，$t=T$時為估計的cost)。<br>
透過重複學習這些紀錄(episodes)，智慧型代理人能透過策略網路(policy network)學習最大化cost的方法。這篇論文使用Proximal Policy Optimization (PPO)的方式，來更新策略網路 $\pi_\theta(a|s)$中的參數$\theta$。</p>
<h3 id="獎勵-reward">獎勵 (reward)</h3>
<p>這篇論文的目標是最佳化PPA，而這些資訊通常是由商業tool在flow的最後產生，也就是report QoR，包含線長、繞線擁擠狀況、timing、密度、面積、power等等，這些資訊綜合在一起便是最精確的reward。然而這通常得花上數小時甚至一天的時間來產生這些結果，RL需要快速迭代的特性注定讓使用這樣的reward不現實可行，RL需要的是一個能快速估算的reward、同時足夠精確而可以代表最終的QoR。<br>
這篇論文採用線長與繞線擁擠度來估計擺置品質，並將其加權相加後當作reward，也就是$r_t=-HPWL-c*Congestion$。<br>
同時為了保證在足夠短的時間能算出線長，論文做了幾個假設：</p>
<ol>
<li>將標準邏輯閘用hMETIS叢集起來，再用forced-directed方式來快速擺置，這使得計算線長時能獲得快速而不失精確的標準邏輯閘線長。</li>
<li>將平面離散化成數千個格子，並將macro與cell叢集的重心放置於格子的中心。</li>
<li>計算標準邏輯閘時，假設所有的線都是來自叢集中心。</li>
<li>計算繞線壅擠時，只考慮前10%嚴重的格子。</li>
</ol>
<h4 id="線長">線長</h4>
<p>HPWL，常見的估算方法，其意義可以視為是繞線長度所需的下限，大約等於其Steiner tree的線長。
這篇論文並加入一個normalize factor $q(i)$來增加估算的精確性，隨著netlist中結點的數量增加HPWL，如下所示，其中$N_{netlist}$代表netlist的數量：<br>
$HPWL(netlist)={\displaystyle\sum_{i=1}^{N_{netlist}}q(i)*HPWL(i)}$</p>
<h4 id="格子的數量">格子的數量</h4>
<p>對於空白的擺置空間，有無數種格子切法可以將其離散化。而切的數量嚴重影響最佳化的複雜度與解的品質。論文將數量上限定為128x128，並將數量選擇視為一個bin-packing問題，透過浪費的格子數來衡量選擇的好壞。實驗中使用的數量平均約30。</p>
<h4 id="macro擺放順序">Macro擺放順序</h4>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/eda/">eda</a>
        
            <a href="/tags/AI/">AI</a>
        
            <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css"integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js"integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8"crossorigin="anonymous"
                defer="true"
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js"integrity="sha384-vZTG03m&#43;2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl"crossorigin="anonymous"
                defer="true"
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.querySelector(`.article-content`), {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ]
        });})
</script>
    
</article>

    <aside class="related-contents--wrapper">
    
    
        <h2 class="section-title">Related contents</h2>
        <div class="related-contents">
            <div class="flex article-list--tile">
                
                    
<article class="">
    <a href="/EDA/cudnnSetup/">
        
        

        <div class="article-details">
            <h2 class="article-title"># fatal error: FreeImage.h: No such file or directory #include “FreeImage.h” CentOS 7</h2>
        </div>
    </a>
</article>
                
                    
<article class="">
    <a href="/EDA/cplexTroubleshooting/">
        
        

        <div class="article-details">
            <h2 class="article-title">trouble shooting of Concert exception--has not been extracted by IloAlgorithm</h2>
        </div>
    </a>
</article>
                
                    
<article class="">
    <a href="/EDA/corner_stitching/">
        
        

        <div class="article-details">
            <h2 class="article-title">Corner Stitching---A Data-Structuring Technique for VLSI Layout Tools</h2>
        </div>
    </a>
</article>
                
            </div>
        </div>
    
</aside>

     
     
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2017 - 
        
        2022 竹林七閑
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.2.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer="true"
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer="true"
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css"integrity="sha256-c0uckgykQ9v5k&#43;IqViZOZKc47Jn7KQil4/MP3ySA3F8="crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css"integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE="crossorigin="anonymous"
            >

            </main>
    
        <aside class="sidebar right-sidebar sticky">
            <section class="widget archives">
                <div class="widget-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



                </div>
                <h2 class="widget-title section-title">Table of contents</h2>
                
                <div class="widget--toc">
                    <nav id="TableOfContents">
  <ol>
    <li><a href="#引言">引言</a></li>
    <li><a href="#相關研究">相關研究</a></li>
    <li><a href="#方法">方法</a>
      <ol>
        <li><a href="#問題定義">問題定義</a></li>
        <li><a href="#方法概覽">方法概覽</a></li>
        <li><a href="#獎勵-reward">獎勵 (reward)</a>
          <ol>
            <li><a href="#線長">線長</a></li>
            <li><a href="#格子的數量">格子的數量</a></li>
            <li><a href="#macro擺放順序">Macro擺放順序</a></li>
          </ol>
        </li>
      </ol>
    </li>
  </ol>
</nav>
                </div>
            </section>
        </aside>
    

        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g="crossorigin="anonymous"
                defer="false"
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
