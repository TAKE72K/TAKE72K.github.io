<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning on 竹林七閑</title>
    <link>https://take72k.github.io/tags/Reinforcement-Learning/</link>
    <description>Recent content in Reinforcement Learning on 竹林七閑</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-tw</language>
    <lastBuildDate>Thu, 03 Nov 2022 21:01:00 +0000</lastBuildDate><atom:link href="https://take72k.github.io/tags/Reinforcement-Learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Chip Placement with Deep Reinforcement Learning 閱讀筆記</title>
      <link>https://take72k.github.io/EDA/placementByRL/</link>
      <pubDate>Thu, 03 Nov 2022 21:01:00 +0000</pubDate>
      
      <guid>https://take72k.github.io/EDA/placementByRL/</guid>
      <description>Chip Placement with Deep Reinforcement Learning 閱讀筆記  link
 引言 Google這篇一出可說語驚四座，從未有人把強化學習拿來擺physical design中的macro過。 事實上，這個步驟可說是整個flow中複雜度最高的部分之一，原因無他，所有的改變都牽一髮而動全身，要完美衡量擺放macro的效果去做決定，不能說不可能，但肯定是相當耗時的。 這時工程師真正的作用就出來了，經驗老到的可能一看就知道怎樣擺(可能)比較好，過去我們開發的演算法，可說是在用程式模擬這種經驗；而強化學習則另闢蹊徑，讓AI來扮演菜鳥工程師，從零開始學習這些know-how。
相關研究 Global placment問題是晶片設計上的一大挑戰，前人所提出的方法大致可以歸納為三個方向：基於分區、基於隨機優化、基於分析。
方法 這份研究的課題是晶片擺置最佳化，目標是將netlist(描述晶片的graph)這張圖的節點定位在2D的有限平面上，也就是擺置空間，並且同時最佳化Power, performance and area (PPA)。 為了與深度強化學習做結合，必須將這個擺置問題轉化為Markov Decision Processes (MDPs)的形式，由四個要素構成：
 states: 所有可能發生的環境的集合 (也就是所有可能產生的擺置)。 actions: 智慧型代理人能執行的動作的集合 (也就是擺放macro到一個合法位置)。 state transition: 給定state和action，轉場到下一個state。 reward: 在某個state做某個動作的獎勵 (設定為零，擺完最後一步之後會用估計線長與擁擠狀況的負加權合作為獎勵。)  </description>
    </item>
    
  </channel>
</rss>
